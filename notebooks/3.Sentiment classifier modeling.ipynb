{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "##\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>likes</th>\n",
       "      <th>text length</th>\n",
       "      <th>polarity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>target</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1419307341560745985</td>\n",
       "      <td>face bound border two The covid said hidden</td>\n",
       "      <td>2021-07-25 17:43:34 EAT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-0.1666</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1419307340680114179</td>\n",
       "      <td>I seeing looking like second global going</td>\n",
       "      <td>2021-07-25 17:43:34 EAT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1419307335982346240</td>\n",
       "      <td>sentence people intensive care</td>\n",
       "      <td>2021-07-25 17:43:33 EAT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1419307331599339521</td>\n",
       "      <td>contagious delta variant surging across nation...</td>\n",
       "      <td>2021-07-25 17:43:31 EAT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1419307315220475908</td>\n",
       "      <td>sentence people intensive care</td>\n",
       "      <td>2021-07-25 17:43:28 EAT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1419307341560745985        face bound border two The covid said hidden   \n",
       "1  1419307340680114179          I seeing looking like second global going   \n",
       "2  1419307335982346240                     sentence people intensive care   \n",
       "3  1419307331599339521  contagious delta variant surging across nation...   \n",
       "4  1419307315220475908                     sentence people intensive care   \n",
       "\n",
       "                created_at  likes  text length  polarity sentiment  target  \\\n",
       "0  2021-07-25 17:43:34 EAT    0.0         43.0   -0.1666  negative      -1   \n",
       "1  2021-07-25 17:43:34 EAT    0.0         41.0    0.0000   neutral       0   \n",
       "2  2021-07-25 17:43:33 EAT    0.0         30.0    0.0000   neutral       0   \n",
       "3  2021-07-25 17:43:31 EAT    0.0         98.0    0.0000   neutral       0   \n",
       "4  2021-07-25 17:43:28 EAT    0.0         30.0    0.0000   neutral       0   \n",
       "\n",
       "  flag user  \n",
       "0  NaN  NaN  \n",
       "1  NaN  NaN  \n",
       "2  NaN  NaN  \n",
       "3  NaN  NaN  \n",
       "4  NaN  NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data\n",
    "##\n",
    "tweets = pd.read_csv(\"csv files/ALL_DATA_V2.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the missing data points row-wise\n",
    "##\n",
    "tweets.dropna(axis=0, subset=['text'], inplace=True)\n",
    "tweets.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOW (bag of words)\n",
    "##\n",
    "cv = CountVectorizer(analyzer='word', stop_words=stop)\n",
    "\n",
    "cv.fit(tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "688877\n"
     ]
    }
   ],
   "source": [
    "# Preview vocabulary and the number of vocab words\n",
    "##\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['face',\n",
       " 'bound',\n",
       " 'border',\n",
       " 'two',\n",
       " 'covid',\n",
       " 'said',\n",
       " 'hidden',\n",
       " 'seeing',\n",
       " 'looking',\n",
       " 'like']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 112935)\t1\n",
      "  (0, 113837)\t1\n",
      "  (0, 161356)\t1\n",
      "  (0, 223630)\t1\n",
      "  (0, 276001)\t1\n",
      "  (0, 531134)\t1\n",
      "  (0, 629523)\t1\n"
     ]
    }
   ],
   "source": [
    "# Example transforming a single text\n",
    "##\n",
    "print(cv.transform([tweets['text'][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0c6abp'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example getting the feature name by index\n",
    "##\n",
    "cv.get_feature_names()[759]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the whole BOW to a sparse matrix\n",
    "##\n",
    "bow_text = cv.transform(tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "935370"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non-zero occurrences\n",
    "##\n",
    "bow_text.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf weighting\n",
    "##\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "tfidf_transformer.fit(bow_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17355)\t0.36382217854753895\n",
      "  (0, 14324)\t0.33536544967797327\n",
      "  (0, 7589)\t0.4312292397319281\n",
      "  (0, 5910)\t0.3748466201388867\n",
      "  (0, 3673)\t0.2626363014015341\n",
      "  (0, 1926)\t0.4334212820075949\n",
      "  (0, 1889)\t0.4145635743995186\n"
     ]
    }
   ],
   "source": [
    "# Example transforming a single bow\n",
    "##\n",
    "print(tfidf_transformer.transform(cv.transform([tweets['text'][0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.147106787035735"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example getting idf weight of a word\n",
    "##\n",
    "tfidf_transformer.idf_[cv.vocabulary_['good']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the whole sparse matrix\n",
    "##\n",
    "tfidf_text = tfidf_transformer.transform(bow_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding 'text length' and 'word count' as features to the model  \n",
    "We'll stack the features to the sparse matrix horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Transforming the two columns into sparse matrices\\n##\\ntxt_length = coo_matrix(tweets[\\'text length\\']).reshape(1182,1)\\nwrd_cnt = coo_matrix(tweets[\\'word count\\']).reshape(1182,1)\\n\\nfeatures = hstack([tfidf_text, txt_length, wrd_cnt])\\n\\n# Preview difference in shapes\\nprint(\"Shape of text column sparse matrix: \", tfidf_text.shape)\\nprint(\"Shape of concatenated features sparse matrix: \", features.shape)'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Transforming the two columns into sparse matrices\n",
    "##\n",
    "txt_length = coo_matrix(tweets['text length']).reshape(1182,1)\n",
    "wrd_cnt = coo_matrix(tweets['word count']).reshape(1182,1)\n",
    "\n",
    "features = hstack([tfidf_text, txt_length, wrd_cnt])\n",
    "\n",
    "# Preview difference in shapes\n",
    "print(\"Shape of text column sparse matrix: \", tfidf_text.shape)\n",
    "print(\"Shape of concatenated features sparse matrix: \", features.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tfidf_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing different classification models:  \n",
    "- Logistic Regression\n",
    "- Linear SVC (svm)\n",
    "- SGD Classifier\n",
    "- Random Forest Classifier\n",
    "- Xgboost Classifier\n",
    "- LGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test splits\n",
    "##\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, tweets['target'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "##\n",
    "lr = LogisticRegression(C=2.0, class_weight=None, dual=False, max_iter=100)\n",
    "\n",
    "svc = LinearSVC(C=2.0, class_weight=None, dual=False, max_iter=100)\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "rfc = RandomForestClassifier(bootstrap=False, class_weight=None, \n",
    "                             criterion='entropy', min_samples_split=6, n_estimators=160, warm_start=False)\n",
    "\n",
    "xgb = XGBClassifier(objective='multi:softmax', num_class=3)\n",
    "\n",
    "lgbm = LGBMClassifier(objective='multiclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "##\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "xgb.fit(X_train,y_train)\n",
    "\n",
    "lgbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "##\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "svc_pred = svc.predict(X_test)\n",
    "\n",
    "sgd_pred = sgd.predict(X_test)\n",
    "\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "\n",
    "lgbm_pred = lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression \n",
      "----------------------------------\n",
      "[[ 4211   307   297]\n",
      " [   40 11296    75]\n",
      " [  183   318  9004]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.95      0.87      0.91      4815\n",
      "           0       0.95      0.99      0.97     11411\n",
      "           1       0.96      0.95      0.95      9505\n",
      "\n",
      "    accuracy                           0.95     25731\n",
      "   macro avg       0.95      0.94      0.94     25731\n",
      "weighted avg       0.95      0.95      0.95     25731\n",
      "\n",
      "Linear SVC \n",
      "---------------------------------\n",
      "[[ 4489   109   217]\n",
      " [   53 11289    69]\n",
      " [  154   109  9242]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.93      0.94      4815\n",
      "           0       0.98      0.99      0.99     11411\n",
      "           1       0.97      0.97      0.97      9505\n",
      "\n",
      "    accuracy                           0.97     25731\n",
      "   macro avg       0.97      0.96      0.97     25731\n",
      "weighted avg       0.97      0.97      0.97     25731\n",
      "\n",
      "SGD \n",
      "---------------------------------\n",
      "[[ 3304  1039   472]\n",
      " [   22 11308    81]\n",
      " [  220  1144  8141]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.69      0.79      4815\n",
      "           0       0.84      0.99      0.91     11411\n",
      "           1       0.94      0.86      0.89      9505\n",
      "\n",
      "    accuracy                           0.88     25731\n",
      "   macro avg       0.90      0.84      0.86     25731\n",
      "weighted avg       0.89      0.88      0.88     25731\n",
      "\n",
      "Random Forest Classifier \n",
      "----------------------------------\n",
      "[[ 3971   232   612]\n",
      " [   29 11297    85]\n",
      " [  263   245  8997]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.82      0.87      4815\n",
      "           0       0.96      0.99      0.97     11411\n",
      "           1       0.93      0.95      0.94      9505\n",
      "\n",
      "    accuracy                           0.94     25731\n",
      "   macro avg       0.94      0.92      0.93     25731\n",
      "weighted avg       0.94      0.94      0.94     25731\n",
      "\n",
      "Xgboost Classifier \n",
      "----------------------------------\n",
      "[[ 3415  1016   384]\n",
      " [   31 11324    56]\n",
      " [  237  1376  7892]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.71      0.80      4815\n",
      "           0       0.83      0.99      0.90     11411\n",
      "           1       0.95      0.83      0.88      9505\n",
      "\n",
      "    accuracy                           0.88     25731\n",
      "   macro avg       0.90      0.84      0.86     25731\n",
      "weighted avg       0.89      0.88      0.88     25731\n",
      "\n",
      "LGBM \n",
      "---------------------------------\n",
      "[[ 3920   574   321]\n",
      " [   33 11300    78]\n",
      " [  220   651  8634]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.94      0.81      0.87      4815\n",
      "           0       0.90      0.99      0.94     11411\n",
      "           1       0.96      0.91      0.93      9505\n",
      "\n",
      "    accuracy                           0.93     25731\n",
      "   macro avg       0.93      0.90      0.92     25731\n",
      "weighted avg       0.93      0.93      0.93     25731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "##\n",
    "print(\"Logistic Regression \\n----------------------------------\")\n",
    "print(confusion_matrix(y_test, lr_pred))\n",
    "print(classification_report(y_test, lr_pred))\n",
    "\n",
    "print(\"Linear SVC \\n---------------------------------\")\n",
    "print(confusion_matrix(y_test, svc_pred))\n",
    "print(classification_report(y_test, svc_pred))\n",
    "\n",
    "print(\"SGD \\n---------------------------------\")\n",
    "print(confusion_matrix(y_test, sgd_pred))\n",
    "print(classification_report(y_test, sgd_pred))\n",
    "\n",
    "print(\"Random Forest Classifier \\n----------------------------------\")\n",
    "print(confusion_matrix(y_test, rfc_pred))\n",
    "print(classification_report(y_test, rfc_pred))\n",
    "\n",
    "print(\"Xgboost Classifier \\n----------------------------------\")\n",
    "print(confusion_matrix(y_test, xgb_pred))\n",
    "print(classification_report(y_test, xgb_pred))\n",
    "\n",
    "print(\"LGBM \\n---------------------------------\")\n",
    "print(confusion_matrix(y_test, lgbm_pred))\n",
    "print(classification_report(y_test, lgbm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97100773, 0.96972406, 0.97372717, 0.96976292, 0.96902449])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(svc, features, tweets['target'], cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression \n",
      "------------------------- \n",
      "score=  0.9492269631769622\n",
      "\n",
      "Linear SVC \n",
      "------------------------- \n",
      "score= 0.9706492731618044\n"
     ]
    }
   ],
   "source": [
    "# Making sure that the models are not overfitting using cross val score metric with 5 folds\n",
    "##\n",
    "print(\"Logistic Regression \\n------------------------- \\nscore= \", cross_val_score(lr, features, tweets['target'], cv=5).mean())\n",
    "print(\"\\nLinear SVC \\n------------------------- \\nscore=\", cross_val_score(svc, features, tweets['target'], cv=5).mean())\n",
    "print(\"\\nRandom Forest Classifier \\n------------------------- \\nscore=\", cross_val_score(rfc, features, tweets['target'], cv=5).mean())\n",
    "print(\"\\nXgboost Classifier \\n------------------------- \\nscore=\", cross_val_score(xgb, features, tweets['target'], cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear SVC performed best\n",
    "# That's a good score having in mind that the model is attempting to predict between three category classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "\n",
    "# retrain\n",
    "pipe = Pipeline([('vectorizer', TfidfVectorizer()), ('classifier', LinearSVC(C=2.0, class_weight=None, dual=False, max_iter=100))])\n",
    "pipe.fit(tweets.text, tweets.target)\n",
    "pickle.dump(pipe, open('saved_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = pickle.load(open('saved_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"I don't know\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.int(pipe.predict([\"I love people.\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
